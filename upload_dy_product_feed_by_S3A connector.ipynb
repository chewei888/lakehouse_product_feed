{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26961c3-7fe4-4d79-a915-fae484f0a317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Copy the raw data to a local temp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46712542-49f2-4a12-947f-4ff9869778ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the ADLS Gen2 folder path containing CSV files\n",
    "csv_silver_path = \"abfss://silver@qydatalake.dfs.core.windows.net/dynamicYeild/productfeed_csv/\"\n",
    "\n",
    "# List all files in the folder and filter for those ending with '.csv'\n",
    "files = [file.name for file in dbutils.fs.ls(csv_silver_path) if file.name.endswith('.csv')]\n",
    "\n",
    "# If no CSV files are found, raise an error to halt execution\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {csv_silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89c663e-b5ab-4155-92ca-9c6920374b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Upload the CSV file from ADLS to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0121a7-4b0a-4e75-9ea4-4ddb5e2f0119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_key    = dbutils.secrets.get(\"azure_key_vault\", \"DY-AWS-S3-KEY\")\n",
    "aws_secret = dbutils.secrets.get(\"azure_key_vault\", \"DY-AWS-S3-SECRET\")\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", aws_key)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", aws_secret)\n",
    "# (optional) set your preferred endpoint if needed\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "# Source path in ADLS\n",
    "source_dir = \"abfss://silver@qydatalake.dfs.core.windows.net/dynamicYeild/productfeed_csv/\"\n",
    "\n",
    "# Read source file from source path\n",
    "df = (\n",
    "  spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(source_dir)          # reads all CSVs in that folder\n",
    ")\n",
    "\n",
    "# Write directly into S3\n",
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .csv(\"s3a://com.dynamicyield.feeds/8776216/productfeed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5deb29bc-ef2f-45a4-81c7-e0187313baa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Validate upload status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c3e6ed-14de-46e5-bfbe-151ffb534abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Row counts match: 384 rows\n✅ Schemas match exactly\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1) Read back the CSV(s) you just wrote\n",
    "s3_path = \"s3a://com.dynamicyield.feeds/8776216/productfeed.csv/\"\n",
    "df_s3   = spark.read.option(\"header\",\"true\").csv(s3_path)\n",
    "\n",
    "# 2) Compare counts between source and S3\n",
    "count_src = df.count()\n",
    "count_s3  = df_s3.count()\n",
    "\n",
    "if count_src != count_s3:\n",
    "    raise ValueError(\n",
    "        f\"Row count mismatch: source={count_src}, s3={count_s3}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"✅ Row counts match: {count_src} rows\")\n",
    "\n",
    "# 3) Check schema equivalence\n",
    "if df.schema != df_s3.schema:\n",
    "    print(\"⚠️ Schemas differ:\")\n",
    "    print(\" Source schema:\", df.schema.simpleString())\n",
    "    print(\" S3    schema:\", df_s3.schema.simpleString())\n",
    "else:\n",
    "    print(\"✅ Schemas match exactly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644225a0-cc6f-4a30-8cb0-6fb7aefcbf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "upload_dy_product_feed_by_S3A connector",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}