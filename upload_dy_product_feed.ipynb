{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a26961c3-7fe4-4d79-a915-fae484f0a317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Copy the raw data to a local temp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46712542-49f2-4a12-947f-4ff9869778ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the ADLS Gen2 folder path containing CSV files\n",
    "csv_silver_path = \"abfss://silver@qydatalake.dfs.core.windows.net/dynamicYeild/productfeed_csv/\"\n",
    "\n",
    "# List all files in the folder and filter for those ending with '.csv'\n",
    "files = [file.name for file in dbutils.fs.ls(csv_silver_path) if file.name.endswith('.csv')]\n",
    "\n",
    "# If no CSV files are found, raise an error to halt execution\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {csv_silver_path}\")\n",
    "\n",
    "# Construct the full source path to the first CSV file (the only one CSV file in the folder)\n",
    "source_file = csv_silver_path + files[0]\n",
    "\n",
    "# Define the local temporary file path on the driver node\n",
    "tmp_file = \"file:///tmp/productfeed.csv\"\n",
    "\n",
    "# Copy the CSV from ADLS to the local temp directory\n",
    "dbutils.fs.cp(source_file, tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c89c663e-b5ab-4155-92ca-9c6920374b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Upload the local temp file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab163e5-f795-4534-923a-ec0f7c0cefff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client using AWS credentials stored in Azure Key Vault\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"DY-AWS-S3-KEY\"),\n",
    "    aws_secret_access_key = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"DY-AWS-S3-SECRET\"),\n",
    "    region_name = \"us-east-1\"\n",
    ")\n",
    "\n",
    "# Define the target S3 bucket and object key (folder path + filename)\n",
    "bucket = \"com.dynamicyield.feeds\"\n",
    "key = \"8776216/productfeed.csv\"\n",
    "\n",
    "# Define the local file path of the CSV to upload\n",
    "local_file = \"/tmp/productfeed.csv\"\n",
    "\n",
    "# Upload the local CSV file to the specified S3 bucket and key\n",
    "s3.upload_file(local_file, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5deb29bc-ef2f-45a4-81c7-e0187313baa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Validate upload status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ac5903-1866-43c8-9e80-2092f9e2dc9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded product feed to DY at:\n2025-07-03 00:36:24+00:00\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "\n",
    "try:\n",
    "    # Attempt to fetch the S3 object’s metadata to confirm it exists\n",
    "    resp = s3.head_object(Bucket=bucket, Key=\"8776216/productfeed.csv\")\n",
    "\n",
    "    # If successful, print a confirmation with the last modified timestamp\n",
    "    print(\"Uploaded product feed to DY at:\")\n",
    "    print(resp[\"LastModified\"])\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # If there’s a ClientError (e.g., 404 Not Found), alert and re-raise the exception\n",
    "    print(\"Failed to upload product feed to DY!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644225a0-cc6f-4a30-8cb0-6fb7aefcbf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "upload_dy_product_feed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}