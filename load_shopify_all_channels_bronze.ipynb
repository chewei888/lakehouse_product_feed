{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afb9507-786f-42d9-88de-db230a6a2094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Define Logging and Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21252179-0542-44eb-9e06-28e82fb393ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import logging\n",
    "\n",
    "# Configure the root logger’s level & format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format= \"%(asctime)s %(levelname)s %(message)s\", # https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Grab the root logger once for the whole module\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "# Define a custom ListHandler. Handler that appends each record to the target list.\n",
    "class ListHandler(logging.Handler):\n",
    "    def __init__(self, target_list):\n",
    "        super().__init__() # under the hood invokes logging.Handler.__init__\n",
    "        self.target = target_list\n",
    "\n",
    "    def emit(self, record): # “record” is the LogRecord created by logging.info()/error()        \n",
    "        utc_dt = datetime.fromtimestamp(\n",
    "            record.created, # a UTC-based POSIX timestamp\n",
    "            tz=ZoneInfo(\"UTC\") # keep in UTC\n",
    "        )\n",
    "\n",
    "        # Append structured log entry\n",
    "        self.target.append({\n",
    "            \"run_ts\": utc_dt,\n",
    "            \"level\": record.levelname,\n",
    "            \"message\": record.getMessage()\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04c9703-cffb-4505-8b15-a79902d69cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4bc1a05-aba2-40cf-b525-f643bbb37055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_shopify_secret(\n",
    "    shop: str           = \"SHOPIFY_US\",\n",
    "    api_version: str    = \"2023-04\",\n",
    "    scope: str          = \"azure_key_vault\",\n",
    "    key: str            = \"SHOPIFYUS-PW\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the Shopify secret from Azure Key Vault\n",
    "    \"\"\"\n",
    "    # Define Shopify US private API connection\n",
    "    shop_name = os.getenv(shop)\n",
    "    shop_url = f'{shop_name}.myshopify.com'\n",
    "    api_version = api_version # the lastest version that can be supported by ShopifyAPI.\n",
    "    private_app_password = dbutils.secrets.get(scope=scope, key=key) # use Azure Key Vault to save this password.\n",
    "\n",
    "    return shop_name, shop_url, api_version, private_app_password\n",
    "\n",
    "import shopify\n",
    "import json\n",
    "import urllib.error\n",
    "\n",
    "def fetch_all_pages(entity: str, query: str, session: shopify.Session, page_size: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Paginate through Shopify GraphQL query, collecting all pages.\n",
    "    \"\"\"\n",
    "    # Log the start    \n",
    "    logging.info(\"Starting Shopify Session for '%s'\", entity)\n",
    "\n",
    "    # Activate the shopify session\n",
    "    shopify.ShopifyResource.activate_session(session)\n",
    "\n",
    "    all_edges = []\n",
    "    cursor = None\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            variables = {\"first\": page_size}\n",
    "            if cursor:\n",
    "                variables[\"after\"] = cursor\n",
    "\n",
    "            try:\n",
    "                # Execute GraphQL query and iterate to the end page\n",
    "                raw = shopify.GraphQL().execute(query, variables=variables)\n",
    "                result = json.loads(raw)\n",
    "            except (urllib.error.HTTPError, ValueError) as e:\n",
    "                logging.error(\"Error fetching/parsing %s page: %s\", entity, e)\n",
    "                break\n",
    "\n",
    "            if \"errors\" in result:\n",
    "                msg = result[\"errors\"][0].get(\"message\", \"(no message)\")\n",
    "                logging.error(\"GraphQL error for %s: %s\", entity, msg)\n",
    "                break                \n",
    "                \n",
    "            # data = result[\"data\"][entity]\n",
    "            data = result.get(\"data\", {}).get(entity)\n",
    "            if data is None:\n",
    "                logging.error(\"No data.%s in response\", entity)\n",
    "                break                \n",
    "\n",
    "            nextPage = data[\"pageInfo\"][\"hasNextPage\"]\n",
    "            items = data[\"edges\"]\n",
    "\n",
    "            all_edges.extend(items)\n",
    "            logging.info(\"Received %s records for %s\", len(items), entity)\n",
    "\n",
    "            if not nextPage:\n",
    "                break\n",
    "            cursor = data[\"pageInfo\"][\"endCursor\"]\n",
    "\n",
    "        logging.info(\"Received all the pages for %s\", entity)\n",
    "\n",
    "    finally:\n",
    "        # Deactivate the shopify session\n",
    "        shopify.ShopifyResource.clear_session()\n",
    "        logging.info(\"Cleared Shopify Session for %s\", entity)\n",
    "\n",
    "    return [edge[\"node\"] for edge in all_edges]\n",
    "\n",
    "def write_to_lakehouse(\n",
    "    data: list            =None, \n",
    "    partition_number: int =1, \n",
    "    medal: str            =\"bronze\",\n",
    "    subfolder_path: str   =\"shopify\", \n",
    "    entity: str           =\"\",\n",
    "    file_format: str      =\"delta\", \n",
    "    write_mode: str       =\"overwrite\", \n",
    "    merge_schema: bool    =True\n",
    "):\n",
    "    \"\"\"\n",
    "    Write a list of JSON records into ADLS and register a Delta table.\n",
    "    \"\"\"\n",
    "    # Check whether the data is empty\n",
    "    if not data:\n",
    "        logging.info(\"No data for %s. Skip write.\", entity)\n",
    "        return\n",
    "    \n",
    "    # # Build a RDD\n",
    "    # json_rdd = sc.parallelize([json.dumps(record) for record in data])\n",
    "\n",
    "    # # Read RDD as json\n",
    "    # df = spark.read.json(json_rdd)\n",
    "\n",
    "    # Build a one-column DataFrame of JSON strings - Shared Cluster can't use RDD, so turn to save json in tmp first\n",
    "    json_list = [(json.dumps(rec),) for rec in data]\n",
    "    schema    = StructType([StructField(\"value\", StringType(), True)])\n",
    "    json_df   = spark.createDataFrame(json_list, schema)\n",
    "\n",
    "    # Write it out as text to a temporary ADLS folder\n",
    "    tmp_dir = (\n",
    "        f\"abfss://{medal}@qydatalake.dfs.core.windows.net/\"\n",
    "        f\"{subfolder_path.strip('/')}/_tmp_json/{entity}/\"\n",
    "    )\n",
    "    json_df.coalesce(1).write.mode(\"overwrite\").text(tmp_dir)\n",
    "\n",
    "    # Read the JSON files back so Spark infers the full nested schema\n",
    "    df = spark.read.json(tmp_dir)\n",
    "\n",
    "    # Define the storage path\n",
    "    path = f\"abfss://{medal}@qydatalake.dfs.core.windows.net/{subfolder_path}/{entity}/\"\n",
    "\n",
    "    # Define the catalog name and table name\n",
    "    subfolder_path = subfolder_path.strip(\"/\")\n",
    "    parts = subfolder_path.split(\"/\")\n",
    "    catalog_name = parts[0]\n",
    "    if parts[1:]:\n",
    "        postfix = \"_\".join(parts[1:])\n",
    "        table_name = f\"{entity}_{postfix}\"\n",
    "    else:\n",
    "        table_name = entity\n",
    "    \n",
    "    # Write into Bronze lakehouse\n",
    "    (\n",
    "        df\n",
    "        .coalesce(partition_number) # data is small (< 250 MB), collapse to one file to avoid metadata scan overhead\n",
    "        .write\n",
    "        .format(file_format)\n",
    "        .mode(write_mode)\n",
    "        .option(\"mergeSchema\", merge_schema) # allows new columns to be added when the table already exists\n",
    "        .option(\"path\", path) # writes the Delta files into ADLS\n",
    "        .saveAsTable(f\"{catalog_name}.{medal}.{table_name}\") # registers (or refreshes) the table in Unity Catalog on Databricks\n",
    "    )\n",
    "\n",
    "    # Clean up the temporary JSON folder\n",
    "    dbutils.fs.rm(tmp_dir, recurse=True)\n",
    "\n",
    "    logging.info(\"Wrote %d records of %s to %s\", len(data), entity, path)\n",
    "\n",
    "def fetch_entity_data(\n",
    "    entity: str, query: str, shop_name: str, shop_url: str, api_version: str, password: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Thread-safe wrapper: fetch items & details, capture logs, return data + logs.\n",
    "        Child‐process worker:\n",
    "        1) creates its own Shopify session,\n",
    "        2) fetches all pages for `entity`,\n",
    "        3) captures INFO‐level logs into a list,\n",
    "        4) returns (shop_name, entity, records, logs).\n",
    "    \"\"\"\n",
    "    # 1) spin up the session\n",
    "    session = shopify.Session(shop_url, api_version, password)\n",
    "\n",
    "    # 2) set up log capturing\n",
    "    entity_logs = []\n",
    "    list_handler = ListHandler(entity_logs)\n",
    "    list_handler.setLevel(logging.INFO)\n",
    "    root_logger.addHandler(list_handler)\n",
    "\n",
    "    try:\n",
    "        # 3) do the fetch by fetch_all_pages\n",
    "        try:\n",
    "            # Fetch full JSON for each “item” in every page (a list of dicts)\n",
    "            records = fetch_all_pages(entity, query, session)\n",
    "        except Exception:\n",
    "            logging.exception(\"Unexpected error fetching %s\", entity)\n",
    "            records = []\n",
    "    finally:\n",
    "        # always remove the log‐capturing handler\n",
    "        root_logger.removeHandler(list_handler)\n",
    "\n",
    "    # 4) return everything—including shop_name so caller knows which folder to write to\n",
    "    return shop_name, entity, records, entity_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a3583ca-36a9-4fbe-8d76-f3bbea1e9569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, StructField, StructType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def log_progress(\n",
    "    log_records: list     =None,\n",
    "    medal: str            =\"bronze\",\n",
    "    subfolder_path: str   =\"shopify\", \n",
    "    entity: str           =\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Persist captured logs into Delta under Files/{medal}/{subfolder_path}/logs/{entity}_log.\n",
    "    \"\"\"\n",
    "    if not log_records:\n",
    "        return\n",
    "\n",
    "    # Define the log path\n",
    "    log_path = f\"abfss://{medal}@qydatalake.dfs.core.windows.net/{subfolder_path}/logs/{entity}_log\"\n",
    "\n",
    "    # Define the log schema\n",
    "    schema = StructType([\n",
    "        StructField(\"run_ts\", TimestampType(), nullable=False),\n",
    "        StructField(\"level\", StringType(), nullable=False),\n",
    "        StructField(\"message\", StringType(), nullable=False)\n",
    "    ])\n",
    "\n",
    "    # Create the log dataframe with exact one file output\n",
    "    log_df = spark.createDataFrame(log_records, schema).coalesce(1)\n",
    "\n",
    "    # Write the log\n",
    "    (\n",
    "        log_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", True)\n",
    "        .save(log_path)\n",
    "        # .option(\"path\", log_path)\n",
    "        # .saveAsTable(f\"{subfolder_path}.{medal}.{entity}_log\")\n",
    "    )\n",
    "\n",
    "    logging.info(\"Logged %d entries for %s\", len(log_records), entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b299770b-3c9c-4a42-9e85-982b78bf5cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Compose GraphQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392747a6-ea0e-461b-a7b0-dc0ba1e1afbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GraphQL: $ means variable name. The trailing ! means non‐nullable.\n",
    "product_query = \"\"\"\n",
    "query getProducts($first: Int!, $after: String) {\n",
    "  products(first: $first, after: $after) {\n",
    "    edges {\n",
    "      node {\n",
    "        id\n",
    "        title\n",
    "        bodyHtml\n",
    "        vendor\n",
    "        productType\n",
    "        createdAt\n",
    "        handle\n",
    "        updatedAt\n",
    "        publishedAt\n",
    "        templateSuffix\n",
    "        tags\n",
    "        variants(first: 250) {\n",
    "          edges {\n",
    "            node {\n",
    "              id\n",
    "              sku\n",
    "              title\n",
    "              price\n",
    "              compareAtPrice\n",
    "              image {\n",
    "                url\n",
    "              }\n",
    "              inventoryQuantity\n",
    "              inventoryItem {\n",
    "                id\n",
    "              }\n",
    "              metafields(first: 250) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    id\n",
    "                    namespace\n",
    "                    key\n",
    "                    value\n",
    "                    ownerType\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        metafields(first: 250) {\n",
    "          edges {\n",
    "            node {\n",
    "              id\n",
    "              namespace\n",
    "              key\n",
    "              value\n",
    "              ownerType\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        collections(first: 250) {\n",
    "          edges {\n",
    "            node {\n",
    "              id\n",
    "              handle\n",
    "              title\n",
    "              updatedAt\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    pageInfo {\n",
    "      hasNextPage\n",
    "      hasPreviousPage\n",
    "      startCursor\n",
    "      endCursor\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "inventoryItems_query = \"\"\"\n",
    "query inventoryItems($first: Int!, $after: String)\n",
    "{\n",
    "  inventoryItems(first: $first, after: $after) {\n",
    "    edges {\n",
    "      node {\n",
    "        id\n",
    "        sku\n",
    "        inventoryLevels(first: 250) {\n",
    "          edges {\n",
    "            node {\n",
    "              location {\n",
    "                id\n",
    "                name\n",
    "              }\n",
    "              quantities(names: \"available\") {\n",
    "                name\n",
    "                quantity\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    pageInfo {\n",
    "      hasNextPage\n",
    "      hasPreviousPage\n",
    "      startCursor\n",
    "      endCursor\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170b298f-d4b3-4b2c-94d4-2eba700142f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Fetch entities from Shopify by GraphQL\n",
    "# 2. Write the output to Lakehouse\n",
    "# 3. Log progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc8af4e-9ca8-4fc3-91b4-a8d775560a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\n2025-07-13 20:32:41 INFO Error while sending or receiving.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n2025-07-13 20:32:41 INFO Closing down clientserver connection\n2025-07-13 20:32:41 INFO Exception while sending command.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 530, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending\n2025-07-13 20:32:41 INFO Closing down clientserver connection\n2025-07-13 20:33:26 INFO Error while sending or receiving.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n2025-07-13 20:33:26 INFO Closing down clientserver connection\n2025-07-13 20:33:26 INFO Exception while sending command.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 530, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending\n2025-07-13 20:33:26 INFO Closing down clientserver connection\n2025-07-13 20:35:26 INFO Wrote 453 records of products to abfss://bronze@qydatalake.dfs.core.windows.net/shopify/quayaustralia/products/\n2025-07-13 20:35:27 INFO Logged 49 entries for products\n2025-07-13 20:36:51 INFO Wrote 690 records of products to abfss://bronze@qydatalake.dfs.core.windows.net/shopify/quayusa/products/\n2025-07-13 20:36:52 INFO Logged 72 entries for products\n2025-07-13 20:37:55 INFO Wrote 958 records of inventoryItems to abfss://bronze@qydatalake.dfs.core.windows.net/shopify/quayaustralia/inventoryItems/\n2025-07-13 20:37:56 INFO Logged 99 entries for inventoryItems\n2025-07-13 20:42:35 INFO Wrote 1714 records of inventoryItems to abfss://bronze@qydatalake.dfs.core.windows.net/shopify/quayusa/inventoryItems/\n2025-07-13 20:42:36 INFO Logged 175 entries for inventoryItems\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# === MAIN PIPELINE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the shops (US, AU, etc.) up front:\n",
    "    channels = [\"US\", \"AU\"]\n",
    "    shop_configs = []\n",
    "\n",
    "    for channel in channels: \n",
    "        # Prepare Shopify secret\n",
    "        shop, shop_url, api_version, private_app_password = get_shopify_secret(\n",
    "            shop=f\"SHOPIFY_{channel}\", \n",
    "            api_version=\"2023-04\", \n",
    "            scope= \"azure_key_vault\", \n",
    "            key=f\"SHOPIFY{channel}-PW\"\n",
    "        )\n",
    "\n",
    "        shop_configs.append(\n",
    "            {\n",
    "                \"name\": shop, \n",
    "                \"url\": shop_url, \n",
    "                \"version\": api_version, \n",
    "                \"password\": private_app_password\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Queries we want to load\n",
    "    entity_queries = {\n",
    "        \"products\": product_query, \n",
    "        \"inventoryItems\": inventoryItems_query\n",
    "    }\n",
    "\n",
    "    # Create a separate Shopify session for each entity. Each parallel job is isolated from the others.\n",
    "    jobs = [\n",
    "        (\n",
    "            entity, \n",
    "            query,\n",
    "            shop_config[\"name\"],\n",
    "            shop_config[\"url\"], \n",
    "            shop_config[\"version\"], \n",
    "            shop_config[\"password\"]\n",
    "        )\n",
    "        for shop_config in shop_configs\n",
    "        for entity, query in entity_queries.items()\n",
    "    ]\n",
    "\n",
    "    # Parallel GraphQL fetch (processes)\n",
    "    with ProcessPoolExecutor(max_workers=len(entity_queries)*len(shop_configs)) as pool:\n",
    "        futures = {\n",
    "            pool.submit(fetch_entity_data, entity, query, name, url, version, password): name\n",
    "            for entity, query, name, url, version, password in jobs \n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures): # futures is a Dict[Future, str]\n",
    "            # channel = futures[future]\n",
    "            channel, entity, data, logs = future.result()\n",
    "\n",
    "            # Write to Lakehouse\n",
    "            write_to_lakehouse(data=data, partition_number=1, medal=\"bronze\", subfolder_path=f\"shopify/{channel}\", entity=entity)\n",
    "\n",
    "            # Log progress\n",
    "            log_progress(log_records=logs, medal=\"bronze\", subfolder_path=f\"shopify/{channel}\", entity=entity)\n",
    "\n",
    "# --- End of shopify_loader.py ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced17074-c193-429e-a83c-5a384595689a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5131063115942568,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "load_shopify_all_channels_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}